# This file contains the different endpoints your bot can use.

# Server where the models are pulled from.
# https://rasa.com/docs/rasa-pro/production/model-storage#fetching-models-from-a-server

#models:
#  url: http://my-server.com/models/default_core@latest
#  wait_time_between_pulls:  10   # [optional](default: 100)

# Server which runs your custom actions.
# https://rasa.com/docs/rasa-pro/concepts/custom-actions

action_endpoint:
  url: "http://localhost:5055/webhook"

# Tracker store which is used to store the conversations.
# By default the conversations are stored in memory.
# https://rasa.com/docs/rasa-pro/production/tracker-stores

#tracker_store:
#    type: redis
#    url: <host of the redis instance, e.g. localhost>
#    port: <port of your redis instance, usually 6379>
#    db: <number of your database within redis, e.g. 0>
#    password: <password used for authentication>
#    use_ssl: <whether or not the communication is encrypted, default false>

#tracker_store:
#    type: mongod
#    url: <url to your mongo instance, e.g. mongodb://localhost:27017>
#    db: <name of the db within your mongo instance, e.g. rasa>
#    username: <username used for authentication>
#    password: <password used for authentication>

# Event broker which all conversation events should be streamed to.
# https://rasa.com/docs/rasa-pro/production/event-brokers

#event_broker:
#  url: localhost
#  username: username
#  password: password
#  queue: queue

# The lines below activate contextual rephrasing, using the default OpenAI language model.
# Ensure the OPENAI_API_KEY is set to prevent any missing API key errors.
# For more details, refer to the documentation:
# https://rasa.com/docs/rasa-pro/concepts/contextual-response-rephraser
# To enable the rephraser, remove the comment symbols in the lines below.
#nlg:
#   type: rephrase

model_groups:
  - id: gemini_llm
    models:
      - provider: gemini
        model: gemini-1.5-flash
        api_key: ${GEMINI_API_KEY} # Optional, if you want to set the API key in the model configuration.
  - id: gemini_embeddings_model # Este ID deve corresponder ao que você definiu em config.yml
    models:
      - provider: gemini
        # O modelo de embeddings do Gemini. "embedding-001" é o modelo de embeddings do Google mais comum.
        # Pode haver outros, verifique a documentação mais recente da API Gemini para embeddings.
        model: embedding-001
        # É altamente recomendável usar uma variável de ambiente para a API Key.
        # Essa chave API é a do Google Cloud/Google AI Studio, não do Hugging Face.
        api_key: ${GEMINI_API_KEY}
        # Outras configurações específicas para embeddings se necessário.
        # Por exemplo, tipo de tarefa (task_type) se o provedor Gemini do Rasa suportar:
        # kwargs:
        #   task_type: SEMANTIC_SIMILARITY
